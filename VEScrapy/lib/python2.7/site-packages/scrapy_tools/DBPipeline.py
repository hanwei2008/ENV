# -*- coding: utf-8 -*-
#
# DBPipeline.py
#
# Copyright (c) 2015 Chengdu Lanjing Data&Information Co., Ltd
#

from threading import Lock
from sys import getsizeof as sizeof
import traceback

from scrapy import log
from twisted.enterprise import adbapi
from scrapy.exceptions import NotConfigured, CloseSpider

from misc import check_spider_pipeline
from SpiderCache import SpiderCache


__author__ = 'Zhang Jinsong'


class DBPipeline(object):
    """
    Scrapy 数据库存储中间件，需要在 scrapy 工程 settings.py 中配置 DB_DRIVER
    及 DB_CONNECT_ARGS 和/或 DB_CONNECT_KEYWORDS，前者用以指定数据库驱动模块
    名，后二者用以指定数据库连接信息。

    另外，需在 settings.py 中通过 DB_TABLE_MAP 配置 spider 名与所存表名的映射
    关系，若未设置 DB_TABLE_MAP 或某 spider 所欲存的表名无法在其中找到，则使用
    DB_TABLE_DEFAULT 所指定的表名。

    最后，可根据需要配置：
    DB_CACHE_MAX_SIZE_PER_SPIDER: 每个 spider 可使用的以字节数计算的数据库批量
    提交缓存大小，默认为 10 * 1024 * 1024（10 兆）
    DEFAULT_CACHE_MAX_SIZE_PER_SPIDER: 若未设置 DB_CACHE_MAX_SIZE_PER_SPIDER，
    则以此配置作为它的值
    DB_CACHE_MAX_ELEMENTS_PER_SPIDER: 每个 spider 数据库批量提交缓存的最大元素个数，
    默认为 100 个
    DEFAULT_CACHE_MAX_ELEMENTS_PER_SPIDER: 若未设置 DB_CACHE_MAX_ELEMENTS_PER_SPIDER，
    则以此配置作为它的值

    本组件假定 item 对象所有的域名均包含在所欲存入的表的属性名中。
    """

    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler)

    def __init__(self, crawler):
        self.db_pool = None

        db_driver = crawler.settings.get("DB_DRIVER")
        if not db_driver:
            log.msg("No field DB_DRIVER in settings.py!", level=log.CRITICAL)
            raise NotConfigured
        self.db_driver = db_driver
        try:
            self.db_driver_module = __import__(self.db_driver)
        except ImportError:
            log.msg("Can't find database driver: " + self.db_driver + "!", level=log.CRITICAL)
            raise NotConfigured

        db_connect_args = crawler.settings.get("DB_CONNECT_ARGS")
        db_connect_keywords = crawler.settings.get("DB_CONNECT_KEYWORDS")
        if not db_connect_args and not db_connect_keywords:
            log.msg("You must at least set one of the two fields "
                    "DB_CONNECT_ARGS and DB_CONNECT_KEYWORDS in settings.py!",
                    level=log.CRITICAL)
        if db_connect_args and not isinstance(db_connect_keywords, list):
            log.msg("Field DB_CONNECT_ARGS settings.py must be a list!",
                    level=log.CRITICAL)
            raise NotConfigured
        if db_connect_keywords and not isinstance(db_connect_keywords, dict):
            log.msg("Field DB_CONNECT_KEYWORDS settings.py must be a dict!",
                    level=log.CRITICAL)
            raise NotConfigured
        self.db_connect_args = db_connect_args if db_connect_args else []
        self.db_connect_keywords = db_connect_keywords if db_connect_keywords else {}

        db_table_default = crawler.settings.get('DB_TABLE_DEFAULT')
        db_table_map = crawler.settings.get('DB_TABLE_MAP')
        if not db_table_default and not db_table_map:
            log.msg("You must at least set one of the two fields "
                    "DB_TABLE_MAP and DB_TABLE_DEFAULT in settings.py!",
                    level=log.CRITICAL)
            raise NotConfigured
        if db_table_map and not isinstance(db_table_map, dict):
            log.msg("Field DB_TABLE_MAP in settings.py must be a dict!",
                    level=log.CRITICAL)
            raise NotConfigured

        self.db_table = db_table_default
        self.db_table_map = db_table_map if db_table_map else {}

        db_cache_max_size = crawler.settings.get("DB_CACHE_MAX_SIZE_PER_SPIDER")
        if db_cache_max_size is None:
            db_cache_max_size = crawler.settings.get("DEFAULT_CACHE_MAX_SIZE_PER_SPIDER")
        self.db_cache_max_size = db_cache_max_size if db_cache_max_size is not None else 10 * 1024 * 1024

        db_cache_max_len = crawler.settings.get("DB_CACHE_MAX_ELEMENTS_PER_SPIDER")
        if db_cache_max_len is None:
            db_cache_max_len = crawler.settings.get("DEFAULT_CACHE_MAX_ELEMENTS_PER_SPIDER")
        self.db_cache_max_len = db_cache_max_len if db_cache_max_len is not None else 100

        self.crawler = crawler

        self.cache_buffer = {}
        self.locks = {}

    def open_spider(self, spider):
        self.db_pool = adbapi.ConnectionPool(self.db_driver,
                                             *self.db_connect_args,
                                             **self.db_connect_keywords)
        db_table = self.db_table_map.get(spider.name)
        if db_table:
            self.db_table = db_table
        if not self.db_table:
            spider.log("No table associated with " + spider.name + "!", level=log.CRITICAL)
            raise CloseSpider

        if self.db_cache_max_len > 0:
            max_len = self.db_cache_max_len * 2
        else:
            max_len = 2
        self.cache_buffer[spider.name] = SpiderCache(maxlen=max_len)
        self.locks[spider.name] = Lock()

    def close_spider(self, spider):
        try:
            self.db_pool.runInteraction(self._conditional_insert, None, spider, True)
        finally:
            if self.db_pool:
                self.db_pool.close()
            self.db_pool = None
            self.cache_buffer[spider.name].clear()

    @check_spider_pipeline
    def process_item(self, item, spider):
        query = self.db_pool.runInteraction(self._conditional_insert, item, spider, False)
        query.addErrback(self.handle_error)
        return item

    @staticmethod
    def _generate_sql_statement(item, db_table):
        keys = item.keys()
        prefix = "INSERT INTO {db_table} (".format(db_table=db_table)
        fields = ",".join(keys)
        suffix = ") VALUES (" + "%s," * (len(keys) - 1) + "%s)"
        return prefix + fields + suffix

    def _conditional_insert(self, transaction, item, spider, close_spider):
        lock = self.locks[spider.name]
        lock.acquire()

        cache_queue = self.cache_buffer[spider.name]
        if not close_spider:
            cache_queue.append(item)

        cache_len = len(cache_queue)
        cache_size = sizeof(cache_queue)

        if (close_spider and cache_len > 0) or cache_len > self.db_cache_max_len \
                or (cache_len > 0 and cache_size >= self.db_cache_max_size):
            # 按键类型对 item 进行分类
            key_types = set((tuple(i.keys()) for i in cache_queue))
            item_groups = ([i for i in cache_queue if tuple(i.keys()) == key_type] for key_type in key_types)

            for items in item_groups:
                try:
                    if len(items) > 0:
                        sql_statement = self._generate_sql_statement(items[0], self.db_table)
                        transaction.executemany(sql_statement, (i.values() for i in items))
                        spider.log(
                            "{stored_num} items of size {stored_size} byte(s) stored in db".format(
                                stored_num=len(items),
                                stored_size=sizeof(SpiderCache(items))),
                            level=log.INFO)
                except self.db_driver_module.Error, e:
                    trace_info = traceback.format_exc()
                    spider.log(
                        "Error related db occurred, store in db failed: {message}\n{trace_info}".format(
                            message=e.message,
                            trace_info=trace_info),
                        level=log.ERROR)
                    continue
                except Exception, e:
                    trace_info = traceback.format_exc()
                    spider.log(
                        "Error not related db occurred, store in db failed {message}\n{trace_info}".format(
                            message=e.message,
                            trace_info=trace_info),
                        level=log.ERROR)
                    continue
            cache_queue.clear()
        lock.release()

    @staticmethod
    def handle_error(e):
        log.err(e)
